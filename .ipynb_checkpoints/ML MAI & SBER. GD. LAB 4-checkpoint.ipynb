{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-7QbAnE-T5CZ",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### О задании\n",
    "\n",
    "В данном задании необходимо реализовать обучение линейной регрессии с помощью различных вариантов градиентного спуска.\n",
    "\n",
    "Баллы даются за выполнение отдельных пунктов (Максимальное количество баллов за эту Л.Р. - 5)\n",
    "\n",
    "Задачи в рамках одного раздела рекомендуется решать в том порядке, в котором они даны в задании.\n",
    "\n",
    "Неэффективная реализация кода может негативно отразиться на оценке.\n",
    "Также оценка может быть снижена за плохо читаемый код и плохо считываемые диаграммы.\n",
    "\n",
    "Все ответы должны сопровождаться кодом или комментариями о том, как они были получены.\n",
    "\n",
    "\n",
    "### Формат сдачи\n",
    "Задания сдаются через lms. Вы прикрепляете **ССЫЛКУ НА ПУБЛИЧНЫЙ РЕПОЗИТОРИЙ**, где выполнено ваше задание.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lrv2xhEZT5Cb",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Приготовьтесь, потому что на лекциях и семинарах мы уже прошли через джунгли теории, оттачивая наши навыки в мастерстве оптимизации функционалов. Мы погружались в глубины градиентного спуска, изучая его в каждом возможном амплуа — от классического полного градиента до беспощадного стохастического градиента, не забывая про метод с импульсом, который как боксерский удар прорывается сквозь проблемы оптимизации.\n",
    "\n",
    "Теперь же перед вами стоит вызов, который не для слабонервных. Ваша миссия, если вы, конечно, осмелитесь ее принять, — взять в арсенал четыре разнообразных вида градиентного спуска и смастерить из них инструмент, способный расправиться с любой задачей. Вам предстоит создать собственную версию линейной регрессии, такую, что даже самые опытные аналитики данных будут смотреть на нее с завистью. Испытайте на реальных данных весь арсенал вашего градиентного спуска, сравните, какой из них выходит на арену оптимизации как несокрушимый чемпион.\n",
    "\n",
    "Это задание не для тех, кто привык стоять в сторонке. Это ваш момент славы, ваш шанс выйти на арену, где вашим оружием будет код, а противниками — самые коварные задачи машинного обучения. Покажите, на что вы способны, и пусть ваш код станет легендой!\n",
    "\n",
    "@GPT-4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vYJ6DgC1T5Cb",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Задание 1. Реализация градиентного спуска (1.75 балла)\n",
    "\n",
    "В этом задании вам предстоит написать собственные реализации различных подходов к градиентному спуску с опорой на подготовленные шаблоны в файле `descents.py`.\n",
    "\n",
    "**Все реализуемые методы должны быть векторизованы!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fcQobFLwT5Cc",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Лирическое размышление № 1\n",
    "\n",
    "Ключевая характеристика антиградиента заключается в том, что он направлен к самому быстрому уменьшению значения функции в конкретной точке. Исходя из этого, разумным подходом будет начать движение с определенной точки, переместиться в направлении антиградиента, затем вновь вычислить антиградиент, совершить движение и продолжать таким образом. Давайте опишем этот процесс более формализованно.\n",
    "\n",
    "Предположим, что $w_0$ – это исходный набор параметров (к примеру, набор из нулей или полученный из какого-либо случайного распределения). В этом случае простой градиентный спуск предполагает выполнение следующих действий до достижения сходимости:\n",
    "\n",
    "$$\n",
    "    w_{k + 1} = w_{k} - \\eta_{k} \\nabla_{w} Q(w_{k}).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kqp8PYhcT5Cc",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Лирическое размышление № 2\n",
    "\n",
    "### Задание 0.0. Градиент MSE в матричном виде (0 баллов).\n",
    "\n",
    "Напомним, что функция потерь MSE записывается в матричном виде как:\n",
    "\n",
    "Давайте найдем градиент функции $Q(w)$ по $w$, используя матричное дифференцирование. Функция $Q(w)$ определена как:\n",
    "\n",
    "$$\n",
    "Q(w) = \\frac{1}{\\ell} (y - Xw)^T (y - Xw)\n",
    "$$\n",
    "\n",
    "где:\n",
    "- $Q(w)$ — функция потерь,\n",
    "- $\\ell$ — количество наблюдений\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aWR1zIF0T5Cc"
   },
   "source": [
    "- $y$ — вектор истинных значений,\n",
    "- $X$ — матрица признаков,\n",
    "- $w$ — вектор весов.\n",
    "\n",
    "Градиент функции потерь $Q(w)$ по $w$ находится следующим образом:\n",
    "\n",
    "1. Раскроем скобки в выражении для $Q(w)$:\n",
    "   \n",
    "$$\n",
    "Q(w) = \\frac{1}{\\ell} (y^Ty - y^TXw - w^TX^Ty + w^TX^TXw)\n",
    "$$\n",
    "\n",
    "2. Заметим, что $y^TXw$ и $w^TX^Ty$ представляют собой скаляры и равны между собой. Тогда выражение упрощается до:\n",
    "   \n",
    "$$\n",
    "Q(w) = \\frac{1}{\\ell} (y^Ty - 2y^TXw + w^TX^TXw)\n",
    "$$\n",
    "\n",
    "3. Теперь дифференцируем $Q(w)$ по $w$. При дифференцировании $y^Ty$ как константа относительно $w$ исчезает, а дифференциация оставшейся части дает:\n",
    "\n",
    "$$\n",
    "\\nabla_w Q(w) = \\frac{1}{\\ell} (-2X^Ty + 2X^TXw)\n",
    "$$\n",
    "\n",
    "4. Упростим выражение, вынеся 2 за скобки:\n",
    "\n",
    "$$\n",
    "\\nabla_w Q(w) = \\frac{2}{\\ell} (X^TXw - X^Ty)\n",
    "$$\n",
    "\n",
    "Таким образом, градиент функции потерь $Q(w)$ по вектору весов $w$ равен:\n",
    "\n",
    "$$\n",
    "\\nabla_w Q(w) = \\frac{2}{\\ell} (X^TXw - X^Ty)\n",
    "$$\n",
    "\n",
    "Это выражение и есть искомый градиент."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mMWb49rzT5Cc",
    "jp-MarkdownHeadingCollapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Задание 1.1. Родительский класс BaseDescent (0.25 балла).\n",
    "\n",
    "Реализуйте функции `calc_loss` (вычисление MSE для переданных $x$ и $y$) и `predict` (предсказание $y_{pred}$ для переданных $x$) в классе `BaseDescent`.\n",
    "\n",
    "Все вычисления должны быть векторизованы."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pvvK9aQaT5Cc",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Задание 1.2. Полный градиентный спуск VanillaGradientDescent (0.25 балла).\n",
    "\n",
    "Реализуйте полный градиентный спуск заполнив пропуски в классе `VanillaGradientDescent` в файле `descents.py`. Для вычисления градиента используйте формулу выше. Шаг оптимизации:\n",
    "\n",
    "$$\n",
    "    w_{k + 1} = w_{k} - \\eta_{k} \\nabla_{w} Q(w_{k}).\n",
    "$$\n",
    "\n",
    "Здесь и далее функция `update_weights` должна возвращать разницу между $w_{k + 1}$ и $w_{k}$: $\\quad w_{k + 1} - w_{k} = -\\eta_{k} \\nabla_{w} Q(w_{k})$.\n",
    "\n",
    "Во всех методах градиентного спуска мы будем использовать следующую формулу для длины шага:\n",
    "\n",
    "$$\n",
    "    \\eta_{k} = \\lambda \\left(\\dfrac{s_0}{s_0 + k}\\right)^p\n",
    "$$\n",
    "\n",
    "На практике достаточно настроить параметр $\\lambda$, а остальным выставить параметры по умолчанию: $s_0 = 1, \\, p = 0.5.$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_57oF08vT5Cc",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Лирическое размышление № 3\n",
    "\n",
    "Конечно, давайте переформулируем и перепишем ваш текст для лучшего понимания.\n",
    "\n",
    "В контексте задач машинного обучения, обычно функционал ошибки $Q(w)$ можно представить как среднее арифметическое отдельных ошибок на каждом элементе выборки:\n",
    "\n",
    "$$\n",
    "    Q(w) = \\frac{1}{\\ell} \\sum_{i = 1}^{\\ell} q_i(w),\n",
    "$$\n",
    "\n",
    "где каждая функция $q_i(w)$ отражает ошибку на i-ом объекте выборки.\n",
    "\n",
    "Основная сложность применения метода градиентного спуска заключается в необходимости вычисления градиента по всей выборке на каждом шаге. Это может быть особенно затруднительно при работе с большими данными. Однако, для эффективного шага в направлении минимизации функции потерь, абсолютная точность градиента может быть не столь критична.\n",
    "\n",
    "Мы можем приблизить градиент всей функции, используя среднее значение градиентов для случайно выбранной подвыборки объектов:\n",
    "\n",
    "$$\n",
    "    \\nabla_{w} Q(w_{k}) \\approx \\dfrac{1}{|B|} \\sum_{i \\in B} \\nabla_{w} q_{i}(w_{k}),\n",
    "$$\n",
    "\n",
    "где $B$ является подмножеством выборки с случайно выбранными индексами.\n",
    "\n",
    "Этот подход приводит нас к методу **стохастического градиентного спуска**, который значительно упрощает вычисления и ускоряет процесс обучения, особенно на больших данных."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wP2WVO_oT5Cd",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Задание 1.3. Стохастический градиентный спуск StochasticDescent (0.25 балла).\n",
    "\n",
    "Реализуйте стохастический градиентный спуск заполнив пропуски в классе `StochasticDescent`. Для оценки градиента используйте формулу выше (среднее градиентов случайно выбранного батча объектов). Шаг оптимизации:\n",
    "\n",
    "$$\n",
    "    w_{k + 1} = w_{k} - \\eta_{k} \\dfrac{1}{|B|}\\sum\\limits_{i \\in B}\\nabla_{w} q_{i}(w_{k}).\n",
    "$$\n",
    "\n",
    "Размер батча будет являться гиперпараметром метода, семплируйте индексы для батча объектов с помощью `np.random.randint`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iaau6dMYT5Cd",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Лирическое размышление № 4\n",
    "\n",
    "В процессе оптимизации может случиться так, что направление наискорейшего спуска, определенное антиградиентом, будет резко колебаться от одного шага к другому. Это часто происходит, если функция потерь имеет вытянутые уровни, что приводит к тому, что градиент, всегда перпендикулярный этим линиям, меняет свое направление на противоположное при каждом шаге. Эти колебания могут серьезно замедлить сходимость оптимизационного процесса из-за постоянных \"колебаний\" в обратных направлениях. Чтобы сгладить эти осцилляции и ускорить процесс оптимизации, применяется метод усреднения градиентов из нескольких предыдущих шагов, тем самым снижая \"шум\" и выявляя общее предпочтительное направление движения. Это достигается с помощью введения вектора инерции:\n",
    "\n",
    "\\begin{align}\n",
    "    &h_0 = 0, \\\\\n",
    "    &h_{k + 1} = \\alpha h_{k} + \\eta_k \\nabla_w Q(w_{k}),\n",
    "\\end{align}\n",
    "\n",
    "где $\\alpha$ — коэффициент, контролирующий влияние градиентов предыдущих шагов, уменьшая их вклад со временем. Можно использовать аппроксимацию градиента для вычисления $h_{k + 1}$. Для осуществления следующего шага градиентного спуска текущую точку смещают на вектор инерции:\n",
    "\n",
    "$$\n",
    "    w_{k + 1} = w_{k} - h_{k + 1}.\n",
    "$$\n",
    "\n",
    "Такой подход позволяет сгладить колебания градиента: если градиент по какому-то направлению часто меняет знак, его вклад в вектор инерции будет уменьшаться, в то время как постоянное направление градиента приведет к увеличению шага в этом направлении, делая процесс оптимизации более стабильным и направленным."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4UWMm3VUT5Cd",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Задание 1.4 Метод Momentum MomentumDescent (0.25 балла).\n",
    "\n",
    "Реализуйте градиентный спуск с методом инерции заполнив пропуски в классе `MomentumDescent`. Шаг оптимизации:\n",
    "\n",
    "\\begin{align}\n",
    "    &h_0 = 0, \\\\\n",
    "    &h_{k + 1} = \\alpha h_{k} + \\eta_k \\nabla_w Q(w_{k}) \\\\\n",
    "    &w_{k + 1} = w_{k} - h_{k + 1}.\n",
    "\\end{align}\n",
    "\n",
    "$\\alpha$ будет являться гиперпараметром метода, но в данном домашнем задании мы зафиксируем её за вас $\\alpha = 0.9$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ligtCpjmT5Cd",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Лирическо размышление № 5\n",
    "\n",
    "Выбор размера шага играет критическую роль в эффективности градиентного спуска. Слишком большой шаг может привести к тому, что процесс будет \"перепрыгивать\" минимальное значение, а слишком маленький шаг существенно замедлит достижение минимума, требуя большего количества итераций. Предварительно определить идеальный размер шага невозможно, и даже стратегии постепенного его уменьшения могут оказаться неэффективными.\n",
    "\n",
    "AdaGrad предлагает индивидуальный подход к регулированию длины шага для каждой отдельной компоненты параметров. Суть метода заключается в уменьшении размера шага в зависимости от общей длины предыдущих шагов по данному параметру:\n",
    "\n",
    "\\begin{align}\n",
    "    &G_{kj} = G_{k-1,j} + (\\nabla_w Q(w_{k - 1}))_j^2; \\\\\n",
    "    &w_{jk} = w_{j,k-1} - \\frac{\\eta_t}{\\sqrt{G_{kj}} + \\varepsilon} (\\nabla_w Q(w_{k - 1}))_j.\n",
    "\\end{align}\n",
    "\n",
    "Где $\\varepsilon$ — малая добавка для предотвращения деления на ноль.\n",
    "\n",
    "В AdaGrad размер шага может быть фиксирован с самого начала, исключая необходимость его подбора в процессе. Этот метод особенно эффективен в задачах с разреженными данными, где большинство признаков для объектов равны нулю. Таким образом, большие шаги будут совершаться по редко встречающимся признакам, в то время как по часто встречающимся — маленькие.\n",
    "\n",
    "Основной недостаток AdaGrad заключается в неизбежном замедлении шагов из-за монотонного роста $G_{kj}$, что может остановить процесс до достижения оптимального решения. Эту проблему решает метод RMSprop, где применяется экспоненциальное сглаживание для градиентов:\n",
    "\n",
    "$$\n",
    "    G_{kj} = \\alpha G_{k-1,j} + (1 - \\alpha) (\\nabla_w Q(w^{(k-1)}))_j^2.\n",
    "$$\n",
    "\n",
    "Здесь шаг адаптируется в зависимости от интенсивности движения по каждому направлению на недавних итерациях.\n",
    "\n",
    "Объединяя идеи этих методов, можно достичь эффективного накопления информации о градиентах для стабилизации процесса и внедрить адаптивную длину шага для каждого параметра, обеспечивая более сбалансированное и быстрое приближение к минимуму."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iYFN8nzBT5Cd",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Задание 1.5. Метод Adam (Adaptive Moment Estimation) (0.75 балла).\n",
    "\n",
    "Реализуйте градиентный спуск с методом Adam заполнив пропуски в классе `Adam`. Шаг оптимизации:\n",
    "\n",
    "\\begin{align}\n",
    "    &m_0 = 0, \\quad v_0 = 0; \\\\ \\\\\n",
    "    &m_{k + 1} = \\beta_1 m_k + (1 - \\beta_1) \\nabla_w Q(w_{k}); \\\\ \\\\\n",
    "    &v_{k + 1} = \\beta_2 v_k + (1 - \\beta_2) \\left(\\nabla_w Q(w_{k})\\right)^2; \\\\ \\\\\n",
    "    &\\widehat{m}_{k} = \\dfrac{m_k}{1 - \\beta_1^{k}}, \\quad \\widehat{v}_{k} = \\dfrac{v_k}{1 - \\beta_2^{k}}; \\\\ \\\\\n",
    "    &w_{k + 1} = w_{k} - \\dfrac{\\eta_k}{\\sqrt{\\widehat{v}_{k + 1}} + \\varepsilon} \\widehat{m}_{k + 1}.\n",
    "\\end{align}\n",
    "\n",
    "$\\beta_1 = 0.9, \\beta_2 = 0.999$ и $\\varepsilon = 10^{-8}$ будут зафиксированы за вас."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XT2SZZ5dT5Cd",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Задание 2. Реализация линейной регресии (0.25 балла)\n",
    "\n",
    "Ваша задача — создать собственную версию линейной регрессии, которая будет обучаться с помощью метода градиентного спуска, следуя предоставленным шаблонам в файле `linear_regression.py` под классом **LinearRegression**. Главные требования к реализации:\n",
    "\n",
    "- Используйте векторизацию для всех вычислений, минимизируйте использование циклов в Python, за исключением итераций градиентного спуска.\n",
    "- Прекращайте обучение, когда выполнено хотя бы одно из следующих условий:\n",
    "  - Евклидова норма разности векторов весов между двумя последовательными итерациями становится меньше заданного порога `tolerance`.\n",
    "  - В векторе весов появляются значения NaN.\n",
    "  - Достигнуто максимальное количество итераций `max_iter`.\n",
    "- Предполагается, что данные для обучения уже содержат столбец из единиц в качестве последнего столбца, обеспечивающего вектор свободных членов.\n",
    "- Для отслеживания процесса сходимости используйте массив `loss_history`, куда следует записывать значения функции потерь до начала первого шага градиентного спуска и после каждой итерации, включая итоговое значение после завершения обучения."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tr-r2Q7CT5Cd",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Задание 3. Проверка кода (0 баллов)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "AM1oq0kzT5Ce",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "i-lxiKJHT5Ce",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from descents import get_descent\n",
    "from linear_regression import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "EWLILm38T5Cf",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "num_objects = 100\n",
    "dimension = 5\n",
    "\n",
    "x = np.random.rand(num_objects, dimension)\n",
    "y = np.random.rand(num_objects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "E0YvNTWzT5Cf",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying full...\n",
      "Trying stochastic...\n",
      "Trying momentum...\n",
      "Trying adam...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "descent_config = {\n",
    "    'descent_name': 'типа что-то делает',\n",
    "    'kwargs': {\n",
    "        'dimension': dimension\n",
    "    }\n",
    "}\n",
    "\n",
    "for descent_name in ['full', 'stochastic', 'momentum', 'adam']:\n",
    "    print(f\"Trying {descent_name}...\")\n",
    "    descent_config['descent_name'] = descent_name\n",
    "    descent = get_descent(descent_config)\n",
    "\n",
    "    diff = descent.step(x, y)\n",
    "    gradient = descent.calc_gradient(x, y)\n",
    "    predictions = descent.predict(x)\n",
    "\n",
    "    assert gradient.shape[0] == dimension, f'Gradient failed for descent {descent_name}'\n",
    "    assert diff.shape[0] == dimension, f'Weights failed for descent {descent_name}'\n",
    "    assert predictions.shape == y.shape, f'Prediction failed for descent {descent_name}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "uEVnwJVvT5Cf",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# LinearRegression\n",
    "\n",
    "max_iter = 10\n",
    "tolerance = 0\n",
    "\n",
    "descent_config = {\n",
    "    'descent_name': 'stochastic',\n",
    "    'kwargs': {\n",
    "        'dimension': dimension,\n",
    "        'batch_size': 10\n",
    "    }\n",
    "}\n",
    "\n",
    "regression = LinearRegression(\n",
    "    descent_config=descent_config,\n",
    "    tolerance=tolerance,\n",
    "    max_iter=max_iter\n",
    ")\n",
    "\n",
    "regression.fit(x, y)\n",
    "\n",
    "assert len(regression.loss_history) == max_iter + 1, 'Loss history failed'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M6FLAkkyT5Cf",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Задание 4. Работа с данными (0.5 балла)\n",
    "\n",
    "Мы будем использовать датасет объявлений по продаже машин на немецком Ebay. В задаче предсказания целевой переменной для нас будет являться цена."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xnvmIlbuT5Cf",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Вам нужно выполнить базовый EDA анализ:\n",
    "\n",
    "1. **Визуализация распределения целевой переменной**:\n",
    "    - Постройте график распределения целевой переменной, чтобы оценить его форму.\n",
    "    - Если распределение сильно скошено, рассмотрите возможность применения логарифмического преобразования к целевой переменной для нормализации распределения.\n",
    "    - Оцените наличие выбросов, аномально высоких или низких значений целевой переменной, используя графический метод или статистические меры (например, интерквартильный размах).\n",
    "\n",
    "2. **Удаление выбросов**:\n",
    "    - Если в данных присутствуют выбросы с аномальной ценой, удалите их, чтобы они не искажали результаты анализа и моделирования.\n",
    "\n",
    "3. **Исследование данных**:\n",
    "    - Проанализируйте типы данных в столбцах (категориальные, числовые, текстовые и т.д.).\n",
    "    - Постройте графики для анализа зависимости целевой переменной от других признаков. Это поможет понять, какие признаки влияют на целевую переменную.\n",
    "    - Изучите распределения значений признаков для выявления аномалий и выбросов. Определите, какие признаки требуют предварительной обработки или трансформации.\n",
    "    - На основе графиков и анализа определите, какие признаки кажутся полезными для моделирования.\n",
    "\n",
    "4. **Предобработка данных**:\n",
    "    - Определите, какие трансформации данных (например, нормализация, стандартизация, кодирование категориальных переменных) могут быть применены к признакам.\n",
    "    - Разделите признаки на категории: категориальные, числовые (вещественные) и те, которые не требуют предобработки.\n",
    "\n",
    "5. **Разделение данных на выборки**:\n",
    "    - Разделите ваши данные на обучающую, валидационную и тестовую выборки в пропорции 8:1:1. Это важный шаг для оценки производительности модели и избежания переобучения.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eBGv6f8zT5Cf",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from descents import get_descent\n",
    "from linear_regression import LinearRegression\n",
    "\n",
    "sns.set(style='darkgrid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HISB4M-NT5Cg",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('autos.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3Vh3Mh9gT5Cg",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Oz4SPbBgT5Cg",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Колонки в данных:\n",
    "\n",
    "* `brand` - название бренда автомобиля\n",
    "* `model` - название модели автомобиля\n",
    "* `vehicleType` - тип транспортного средства\n",
    "* `gearbox` - тип трансмисcии\n",
    "* `fuelType` - какой вид топлива использует автомобиль\n",
    "* `notRepairedDamage` - есть ли в автомобиле неисправность, которая еще не устранена\n",
    "* `powerPS` - мощность автомобиля в PS (метрическая лошадиная сила)\n",
    "* `kilometer` - сколько километров проехал автомобиль, пробег\n",
    "* `autoAgeMonths` - возраст автомобиля в месяцах\n",
    "\n",
    "\n",
    "* `price` - цена, указанная в объявлении о продаже автомобиля (целевая переменная)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0_MOBhTET5Cg",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "categorical = []\n",
    "numeric = []\n",
    "other = []\n",
    "\n",
    "#╰( ͡☉ ͜ʖ ͡☉ )つ──☆*:・ﾟ   ฅ^•ﻌ•^ฅ   ʕ•ᴥ•ʔ (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L1PbGPzYT5Cg",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data['bias'] = 1\n",
    "other += ['bias']\n",
    "\n",
    "x = data[categorical + numeric + other]\n",
    "y = data['price']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gAS9suGWT5Cg",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "column_transformer = ColumnTransformer([\n",
    "    ('ohe', OneHotEncoder(handle_unknown='ignore'), categorical),\n",
    "    ('scaling', StandardScaler(), numeric),\n",
    "    ('other',  'passthrough', other)\n",
    "])\n",
    "\n",
    "x = column_transformer.fit_transform(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KLld6SSeT5Cg",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#╰( ͡☉ ͜ʖ ͡☉ )つ──☆*:・ﾟ   ฅ^•ﻌ•^ฅ   ʕ•ᴥ•ʔ (data split into train/val/test):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mZUC1CaLT5Cg",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Задание 5. Сравнение методов градиентного спуска (1 балл)\n",
    "\n",
    "В этом задании вам предстоит сравнить методы градиентного спуска на подготовленных вами данных из предыдущего задания."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Oj-CDtIAT5Cg",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Задание 5.1. Подбор оптимальной длины шага (0.5 балла)\n",
    "\n",
    "Процесс выбора наиболее подходящего размера шага $\\lambda$ для различных методов, с учетом валидационного набора данных, предполагает выполнение следующих шагов:\n",
    "\n",
    "1. **Определение диапазона для $\\lambda$**: Начните с выбора диапазона значений $\\lambda$, используя логарифмическую сетку от $10^{-4}$ до $10^1$, чтобы обеспечить широкий охват потенциально оптимальных значений.\n",
    "\n",
    "2. **Перебор значений $\\lambda$**: Для каждого значения из выбранной сетки $\\lambda$:\n",
    "   - Произведите обучение модели на обучающем наборе данных.\n",
    "   - Вычислите ошибку на обучающем и валидационном наборах данных.\n",
    "   - Определите значение метрики $R^2$ как на обучающем, так и на валидационном наборах.\n",
    "   - Зафиксируйте количество итераций, необходимое для достижения сходимости.\n",
    "\n",
    "3. **Оценка полученных результатов**:\n",
    "   - Составьте графики, отображающие зависимость ошибки от количества итераций для каждого значения $\\lambda$ по всем рассматриваемым методам.\n",
    "   - Сравните методы на основе скорости сходимости, размера ошибки и значения метрики $R^2$ на различных наборах данных.\n",
    "\n",
    "4. **Выбор наилучшего $\\lambda$**: Исходя из проведенного анализа, определите наиболее подходящее значение $\\lambda$ для каждого метода, обеспечивающее оптимальное сочетание скорости сходимости и качества модели на валидационной выборке.\n",
    "\n",
    "5. **Формулировка выводов**: Подведите итоги, указав, какой метод показал наилучшую производительность с точки зрения соотношения скорости сходимости к качеству предсказаний. Также отметьте, как изменение $\\lambda$ влияет на результаты каждого из методов.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LzzkJ12eT5Ch",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#╰( ͡☉ ͜ʖ ͡☉ )つ──☆*:・ﾟ   ฅ^•ﻌ•^ฅ   ʕ•ᴥ•ʔ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X8QqbdVzT5Ch",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Задание 5.2. Сравнение методов (0.5 балла)\n",
    "\n",
    "Создайте график, на котором будет показана динамика изменения ошибки на обучающей выборке в зависимости от номера итерации для каждого из рассматриваемых методов. Разместите линии, представляющие каждый метод, на одном и том же графике для наглядного сравнения.\n",
    "\n",
    "После анализа результатов, представленных в виде графика и таблиц с метриками, выполните сравнение методов. Обратите внимание на следующие аспекты:\n",
    "- Как быстро каждый метод сходится к минимуму ошибки.\n",
    "- Величину ошибки на обучающей и тестовой выборках для каждого метода.\n",
    "- Значение метрики $R^2$ для каждого метода на обучающей и тестовой выборках.\n",
    "- Количество итераций, необходимых для достижения сходимости.\n",
    "\n",
    "На основе этих данных сделайте вывод о преимуществах и недостатках каждого из методов в контексте вашей задачи."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0m7KtlWMT5Ch",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#╰( ͡☉ ͜ʖ ͡☉ )つ──☆*:・ﾟ   ฅ^•ﻌ•^ฅ   ʕ•ᴥ•ʔ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iboLWHS_T5Ch",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "╰( ͡☉ ͜ʖ ͡☉ )つ──☆*:・ﾟ   ฅ^•ﻌ•^ฅ   ʕ•ᴥ•ʔ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "POJZrXzDT5Ch",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Задание 6. Стохастический градиентный спуск и размер батча (0.5 балла)\n",
    "\n",
    "Ваша задача — исследовать, как размер батча влияет на процесс обучения при использовании стохастического градиентного спуска (SGD). Выполните следующие шаги:\n",
    "\n",
    "1. **Выбор размеров батча**: Определите ряд значений для размера батча, которые вы хотите исследовать. Это могут быть, например, %1$, $10$, $50$, $100$, $500$, и так далее.\n",
    "\n",
    "2. **Многократные запуски для каждого размера батча**: Для каждого выбранного размера батча проведите $k$ независимых запусков стохастического градиентного спуска на обучающей выборке. $k$ может быть равно, например, $10$. Для каждого запуска замерьте:\n",
    "   - Время обучения в секундах до достижения сходимости.\n",
    "   - Количество итераций (шагов), необходимых для сходимости.\n",
    "\n",
    "3. **Вычисление средних значений**: Рассчитайте среднее время обучения и среднее количество итераций до сходимости для каждого размера батча.\n",
    "\n",
    "4. **Построение графиков**:\n",
    "   - Постройте график, показывающий зависимость среднего количества итераций до сходимости от размера батча.\n",
    "   - Постройте график, показывающий зависимость среднего времени обучения от размера батча.\n",
    "\n",
    "5. **Анализ результатов**: Оцените, как размер батча влияет на скорость и эффективность обучения. Сделайте выводы о том, какой размер батча может быть оптимальным с точки зрения баланса между временем обучения и количеством итераций до сходимости.\n",
    "\n",
    "Этот эксперимент поможет вам лучше понять влияние размера батча на процесс обучения стохастического градиентного спуска и как этот параметр можно настроить для улучшения производительности обучения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m-PhWFVET5Ci",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "batch_sizes = np.arange(5, 500, 10)\n",
    "\n",
    "#╰( ͡☉ ͜ʖ ͡☉ )つ──☆*:・ﾟ   ฅ^•ﻌ•^ฅ   ʕ•ᴥ•ʔ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YUVZvNuJT5Ci",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "╰( ͡☉ ͜ʖ ͡☉ )つ──☆*:・ﾟ   ฅ^•ﻌ•^ฅ   ʕ•ᴥ•ʔ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N6nZS55_T5Cj",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Задание 7. Регуляризация (0.5 балла)\n",
    "\n",
    "В этом задании вам предстоит исследовать влияние регуляризации на работу различных методов градиентного спуска. Напомним, регуляризация - это добавка к функции потерь, которая штрафует за норму весов. Мы будем использовать l2 регуляризацию, таким образом функция потерь приобретает следующий вид:\n",
    "\n",
    "$$\n",
    "    Q(w) = \\dfrac{1}{\\ell} \\sum\\limits_{i=1}^{\\ell} (a_w(x_i) - y_i)^2 + \\dfrac{\\mu}{2} \\| w \\|^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W3jbvy89T5Cj",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Допишите класс **BaseDescentReg** в файле `descents.py`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j1E7bAjUT5Cj",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Найдите лучшие параметры обучения с регуляризацией аналогично 5 заданию. Вам предстоит исследовать, как настройка параметров обучения с включением регуляризации влияет на различные методы градиентного спуска. Основная цель — определить оптимальные значения для длины шага $\\lambda$ и коэффициента регуляризации $\\mu$, а затем сравнить результаты обучения с регуляризацией и без неё по нескольким критериям.\n",
    "\n",
    "Ваш план действий следующий:\n",
    "\n",
    "1. **Выбор параметров для подбора**: Установите диапазон значений для длины шага $\\lambda$ и коэффициента регуляризации $\\mu$. Используйте логарифмическую сетку для обоих параметров, чтобы обеспечить широкий охват потенциально оптимальных значений.\n",
    "\n",
    "2. **Оптимизация и сравнение методов градиентного спуска**:\n",
    "   - Произведите подбор параметров для каждого метода градиентного спуска, исследуя их влияние на процесс обучения.\n",
    "   - Замерьте и сравните ошибку и качество по метрике $R^2$ на обучающей и тестовой выборках, а также количество итераций до сходимости для моделей с регуляризацией и без неё.\n",
    "\n",
    "3. **Визуализация результатов**:\n",
    "   - Постройте для каждого метода графики, отображающие значения функции потерь (MSE) с регуляризацией и без неё на протяжении процесса обучения.\n",
    "\n",
    "4. **Анализ результатов**:\n",
    "   - Оцените, как регуляризация повлияла на сходимость методов.\n",
    "   - Сравните качество моделей на обучающей и тестовой выборках с учетом регуляризации и без неё.\n",
    "   - Проанализируйте, как изменения в длине шага и коэффициенте регуляризации отразились на итоговых результатах.\n",
    "\n",
    "5. **Формулировка выводов**:\n",
    "   - Сделайте выводы о влиянии регуляризации на процесс обучения и качество модели. Как регуляризация влияет на переобучение и обобщающую способность модели.\n",
    "   - Рассмотрите, в каких случаях регуляризация приводит к улучшению результатов, и когда её вклад может быть минимальным или отрицательным.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pA762KhbT5Cj",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#╰( ͡☉ ͜ʖ ͡☉ )つ──☆*:・ﾟ   ฅ^•ﻌ•^ฅ   ʕ•ᴥ•ʔ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hHFWz8RXT5Ck",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "╰( ͡☉ ͜ʖ ͡☉ )つ──☆*:・ﾟ   ฅ^•ﻌ•^ฅ   ʕ•ᴥ•ʔ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uv5IIxaQT5Ck",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Задание 8. Альтернативная функция потерь (0.5 балла)\n",
    "\n",
    "В этом задании вам предстоит использовать другую функцию потерь для нашей задачи регрессии. В качестве функции потерь мы выбрали **Log-Cosh**:\n",
    "\n",
    "$$\n",
    "    L(y, a)\n",
    "    =\n",
    "    \\log({cosh{(a - y)}}).\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mzibZ5S2T5Ck",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Самостоятельно продифференцируйте данную функцию потерь чтобы найти её градиент:\n",
    "\n",
    "╰( ͡☉ ͜ʖ ͡☉ )つ──☆*:・ﾟ   ฅ^•ﻌ•^ฅ   ʕ•ᴥ•ʔ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E2eid0ztT5Ck",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Программно реализуйте градиентный спуск с данной функцией потерь в файле `descents.py`, обучите все четыре метода (без регуляризации) аналогично 5 заданию, сравните их качество с четырьмя методами из 5 задания.\n",
    "\n",
    "Пример того, как можно запрограммировать использование нескольких функций потерь внутри одного класса градиентного спуска:\n",
    "\n",
    "\n",
    "```python\n",
    "from enum import auto\n",
    "from enum import Enum\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class LossFunction(Enum):\n",
    "    MSE = auto()\n",
    "    MAE = auto()\n",
    "    LogCosh = auto()\n",
    "    Huber = auto()\n",
    "\n",
    "...\n",
    "class BaseDescent:\n",
    "    def __init__(self, loss_function: LossFunction = LossFunction.MSE):\n",
    "        self.loss_function: LossFunction = loss_function\n",
    "\n",
    "    def calc_gradient(self, x: np.ndarray, y: np.ndarray) -> np.ndarray:\n",
    "        if self.loss_function is LossFunction.MSE:\n",
    "            return ...\n",
    "        elif self.loss_function is LossFunction.LogCosh:\n",
    "            return ...\n",
    "...\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1zxUsZ5OT5Ck",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#╰( ͡☉ ͜ʖ ͡☉ )つ──☆*:・ﾟ   ฅ^•ﻌ•^ฅ   ʕ•ᴥ•ʔ"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "vscode": {
   "interpreter": {
    "hash": "7500c3e1c7c786e4ba1e4b4eb7588219b4e35d5153674f92eb3a82672b534f6e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
